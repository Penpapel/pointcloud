<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Depth Estimation with TensorFlow.js</title>
  <!-- Include TensorFlow.js and Depth Estimation Model -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.20.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/depth-estimation"></script>
  <!-- Include Three.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
  <style>
    body { margin: 0; overflow: hidden; }
    #three-container { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }
    #loading {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      font-family: Arial, sans-serif;
      font-size: 18px;
      color: #333;
    }
  </style>
</head>
<body>

  <!-- Hidden video and canvas elements -->
  <video id="video" autoplay playsinline style="display: none;"></video>
  <canvas id="canvas" style="display: none;"></canvas>
  <!-- Three.js container -->
  <div id="three-container"></div>

  <!-- Loading indicator -->
  <div id="loading">
    Loading model, please wait...
  </div>

  <script>
    let model;
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');

    const scene = new THREE.Scene();
    const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 1, 5000);
    camera.position.z = 1000;
    const renderer = new THREE.WebGLRenderer({ antialias: true });
    renderer.setSize(window.innerWidth, window.innerHeight);
    document.getElementById('three-container').appendChild(renderer.domElement);

    const geometry = new THREE.BufferGeometry();
    const material = new THREE.PointsMaterial({ size: 2, vertexColors: true });
    const points = new THREE.Points(geometry, material);
    scene.add(points);

    async function loadModel() {
      try {
        model = await depthEstimation.load({
          modelType: 'lite', // Use 'lite' for faster loading and better performance on mobile
        });
        document.getElementById('loading').style.display = 'none';
        startVideo();
      } catch (error) {
        console.error('Error loading the model:', error);
        alert('Failed to load the depth estimation model. Please try again later.');
      }
    }

    function startVideo() {
      navigator.mediaDevices.getUserMedia({ video: { facingMode: 'environment' } })
        .then(stream => {
          video.srcObject = stream;
          video.onloadedmetadata = () => {
            // Set canvas dimensions to match the video
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            startApp();
          };
        })
        .catch(err => {
          console.error('Error accessing camera: ', err);
          alert('Error accessing camera: ' + err.message);
        });
    }

    function startApp() {
      processFrame();
    }

    async function processFrame() {
      // Draw the current frame to the canvas
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

      // Get the image data as a tensor
      const inputImage = tf.browser.fromPixels(canvas);

      // Estimate depth
      const depthMap = await model.predict(inputImage);

      // Convert depth map to array
      const depthData = depthMap.dataSync();

      // Generate and render the point cloud
      generatePointCloud(depthData, inputImage, depthMap.shape);

      // Clean up tensors
      inputImage.dispose();
      depthMap.dispose();

      // Render the scene
      renderer.render(scene, camera);

      // Request the next frame
      requestAnimationFrame(processFrame);
    }

    function generatePointCloud(depthData, inputImage, depthShape) {
      const positions = [];
      const colors = [];

      const [height, width] = depthShape;
      const samplingRate = 4; // Adjust as needed

      for (let y = 0; y < height; y += samplingRate) {
        for (let x = 0; x < width; x += samplingRate) {
          const index = y * width + x;
          const depth = depthData[index];

          // Map depth to a suitable range
          const z = depth * 1000;

          // Get the color from the input image tensor
          const pixel = inputImage.slice([y, x, 0], [1, 1, 3]).dataSync();

          const r = pixel[0];
          const g = pixel[1];
          const b = pixel[2];

          positions.push(x - width / 2);
          positions.push(-(y - height / 2));
          positions.push(-z);

          colors.push(r / 255);
          colors.push(g / 255);
          colors.push(b / 255);
        }
      }

      geometry.setAttribute('position', new THREE.Float32BufferAttribute(positions, 3));
      geometry.setAttribute('color', new THREE.Float32BufferAttribute(colors, 3));

      geometry.attributes.position.needsUpdate = true;
      geometry.attributes.color.needsUpdate = true;
    }

    window.addEventListener('load', async () => {
      await loadModel();
    });

    // Handle window resize
    window.addEventListener('resize', () => {
      camera.aspect = window.innerWidth / window.innerHeight;
      camera.updateProjectionMatrix();
      renderer.setSize(window.innerWidth, window.innerHeight);
    });
  </script>

</body>
</html>
